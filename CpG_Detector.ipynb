{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4T6QHHOnfcQ"
      },
      "source": [
        "# Part 1: Build CpG Detector\n",
        "\n",
        "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
        "\n",
        "We have defined a few helper functions / parameters for performing this task.\n",
        "\n",
        "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
        "\n",
        "A good solution will be a model that can be trained, with high confidence in correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mfS4cLmZD2oB"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence, List\n",
        "from functools import partial\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_f-brPAvKvTn"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE HERE\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# Use this for getting x label\n",
        "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
        "    for i in range(n_seqs):\n",
        "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
        "\n",
        "# Use this for getting y label\n",
        "def count_cpgs(seq: str) -> int:\n",
        "    cgs = 0\n",
        "    for i in range(0, len(seq) - 1):\n",
        "        dimer = seq[i:i+2]\n",
        "        # note that seq is a string, not a list\n",
        "        if dimer == \"CG\":\n",
        "            cgs += 1\n",
        "    return cgs\n",
        "\n",
        "# Alphabet helpers\n",
        "alphabet = 'NACGT'\n",
        "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
        "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
        "\n",
        "intseq_to_dnaseq = partial(map, int2dna.get)\n",
        "dnaseq_to_intseq = partial(map, dna2int.get)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_label_seq(seq: str) -> List:\n",
        "    \"\"\"\n",
        "    we will return a list of 0 and 1 such that:\n",
        "        1. if previous character + current character == \"CG\", we will add 1 to the list\n",
        "        2. if previous character + current character != \"CG\", we will add 0 to the list\n",
        "        3. length of final list will be len(seq) - 1\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for i in range(0, len(seq) - 1):\n",
        "        dimer = seq[i:i+2]\n",
        "        if dimer == \"CG\":\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "8dMv1G1Ltce3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VK9Qg5GHYxOb"
      },
      "outputs": [],
      "source": [
        "# we prepared two datasets for training and evaluation\n",
        "# training data scale we set to 2048\n",
        "# we test on 512\n",
        "\n",
        "def prepare_data(num_samples=100):\n",
        "    # prepared the training and test data\n",
        "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
        "    # step 1\n",
        "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
        "    \"\"\"\n",
        "    hint:\n",
        "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X\n",
        "        2. You first convert ids back to DNA sequence\n",
        "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
        "    \"\"\"\n",
        "    #step2\n",
        "    temp = [\"\".join(list(intseq_to_dnaseq(seq))) for seq in X_dna_seqs_train] # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
        "    #step3\n",
        "    y_dna_seqs = [count_cpgs(seq) for seq in temp] # use count_cpgs here to generate labels with temp generated in step2\n",
        "\n",
        "    custom_y_dna_seqs = [custom_label_seq(seq) for seq in temp]\n",
        "\n",
        "    return X_dna_seqs_train, custom_y_dna_seqs, y_dna_seqs\n",
        "\n",
        "train_x, train_y, train_y_int = prepare_data(2048)\n",
        "test_x, test_y, test_y_int = prepare_data(512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assert that length of train_x, train_y and train_y_int are same\n",
        "for l in range(len(train_x)):\n",
        "    assert len(train_x[l]) == 128 and len(train_y[l]) == 127, print(l)"
      ],
      "metadata": {
        "id": "_H6S1xomsH4-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nTdUz9ehn5nE"
      },
      "outputs": [],
      "source": [
        "# some config\n",
        "LSTM_HIDDEN = 64\n",
        "EMBEDDING_DIM = 16\n",
        "LSTM_LAYER = 1\n",
        "batch_size = 32\n",
        "learning_rate = 0.005\n",
        "epoch_num = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZRcGqge3n5nF"
      },
      "outputs": [],
      "source": [
        "# create data loader\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, custom_labels, int_labels):\n",
        "        self.sequences = sequences\n",
        "        self.custom_labels = custom_labels\n",
        "        self.int_labels = int_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.custom_labels[idx], dtype=torch.float), self.int_labels[idx]\n",
        "\n",
        "\n",
        "train_dataset = SequenceDataset(train_x, train_y, train_y_int)\n",
        "test_dataset = SequenceDataset(test_x, test_y, test_y_int)\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataloader\n",
        "for batch in train_data_loader:\n",
        "    sequence, custom_labels, int_labels = batch\n",
        "    print(sequence.shape, custom_labels.shape, int_labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7_Al5LEBRvG",
        "outputId": "bfa701a8-bd32-480e-ae00-640e4ce65ab4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128]) torch.Size([32, 127]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q8fgxrM0LnLy"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "import torch.nn as nn\n",
        "\n",
        "class CpGPredictor(torch.nn.Module):\n",
        "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
        "        super(CpGPredictor, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear_lstm_layer = nn.Linear(embedding_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        embedded = self.embedding(x) # embedded shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Prepare input for the linear layer: concatenate embeddings of prev and curr tokens\n",
        "        # We need to process pairs of tokens (x[i], x[i+1])\n",
        "        # This means the output will have length seq_len - 1\n",
        "        batch_size, seq_len, embedding_dim = embedded.shape\n",
        "        logits = []\n",
        "\n",
        "        for i in range(1, seq_len):\n",
        "            # Get the embeddings of the current and previous tokens\n",
        "            curr_emb = embedded[:, i, :] # curr_emb shape: (batch_size, embedding_dim)\n",
        "            prev_emb = embedded[:, i-1, :] # prev_emb shape: (batch_size, embedding_dim)\n",
        "\n",
        "            pair_input = torch.cat([prev_emb, curr_emb], dim=1) # pair_input shape: (batch_size, 2 * embedding_dim)\n",
        "            lstm_out = self.linear_lstm_layer(pair_input) # lstm_out shape: (batch_size, 1)\n",
        "            logits.append(lstm_out)\n",
        "\n",
        "        logits = torch.stack(logits, dim=1) # logits shape: (batch_size, seq_len - 1, 1)\n",
        "        return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B66ESTn_n5nH"
      },
      "outputs": [],
      "source": [
        "# init model / loss function / optimizer etc.\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Vocabulary size is 5 (N, A, C, G, T)\n",
        "VOCAB_SIZE = 5\n",
        "\n",
        "model = CpGPredictor(embedding_dim=EMBEDDING_DIM, hidden_dim=LSTM_HIDDEN, vocab_size=VOCAB_SIZE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUjBBuXvn5nH",
        "outputId": "3126e2c8-161f-4d79-a9b7-80f6b9bffd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 0.2589, Avg CpG Sum Difference: 633.1917\n",
            "Epoch 2/50, Train Loss: 0.0591, Avg CpG Sum Difference: 69.0247\n",
            "Epoch 3/50, Train Loss: 0.0307, Avg CpG Sum Difference: 33.8325\n",
            "Epoch 4/50, Train Loss: 0.0179, Avg CpG Sum Difference: 19.8090\n",
            "Epoch 5/50, Train Loss: 0.0112, Avg CpG Sum Difference: 12.4842\n",
            "Epoch 6/50, Train Loss: 0.0075, Avg CpG Sum Difference: 8.3829\n",
            "Epoch 7/50, Train Loss: 0.0053, Avg CpG Sum Difference: 5.9696\n",
            "Epoch 8/50, Train Loss: 0.0039, Avg CpG Sum Difference: 4.3713\n",
            "Epoch 9/50, Train Loss: 0.0030, Avg CpG Sum Difference: 3.3710\n",
            "Epoch 10/50, Train Loss: 0.0023, Avg CpG Sum Difference: 2.6576\n",
            "Epoch 11/50, Train Loss: 0.0019, Avg CpG Sum Difference: 2.1249\n",
            "Epoch 12/50, Train Loss: 0.0016, Avg CpG Sum Difference: 1.7675\n",
            "Epoch 13/50, Train Loss: 0.0013, Avg CpG Sum Difference: 1.4600\n",
            "Epoch 14/50, Train Loss: 0.0011, Avg CpG Sum Difference: 1.2350\n",
            "Epoch 15/50, Train Loss: 0.0009, Avg CpG Sum Difference: 1.0689\n",
            "Epoch 16/50, Train Loss: 0.0008, Avg CpG Sum Difference: 0.9230\n",
            "Epoch 17/50, Train Loss: 0.0007, Avg CpG Sum Difference: 0.7951\n",
            "Epoch 18/50, Train Loss: 0.0006, Avg CpG Sum Difference: 0.7027\n",
            "Epoch 19/50, Train Loss: 0.0006, Avg CpG Sum Difference: 0.6297\n",
            "Epoch 20/50, Train Loss: 0.0005, Avg CpG Sum Difference: 0.5577\n",
            "Epoch 21/50, Train Loss: 0.0004, Avg CpG Sum Difference: 0.5006\n",
            "Epoch 22/50, Train Loss: 0.0004, Avg CpG Sum Difference: 0.4475\n",
            "Epoch 23/50, Train Loss: 0.0004, Avg CpG Sum Difference: 0.4077\n",
            "Epoch 24/50, Train Loss: 0.0003, Avg CpG Sum Difference: 0.3670\n",
            "Epoch 25/50, Train Loss: 0.0003, Avg CpG Sum Difference: 0.3363\n",
            "Epoch 26/50, Train Loss: 0.0003, Avg CpG Sum Difference: 0.3080\n",
            "Epoch 27/50, Train Loss: 0.0003, Avg CpG Sum Difference: 0.2811\n",
            "Epoch 28/50, Train Loss: 0.0002, Avg CpG Sum Difference: 0.2584\n",
            "Epoch 29/50, Train Loss: 0.0002, Avg CpG Sum Difference: 0.2390\n",
            "Epoch 30/50, Train Loss: 0.0002, Avg CpG Sum Difference: 0.2204\n",
            "Epoch 31/50, Train Loss: 0.0002, Avg CpG Sum Difference: 0.2027\n",
            "Epoch 32/50, Train Loss: 0.0002, Avg CpG Sum Difference: 0.1879\n",
            "Epoch 33/50, Train Loss: 0.0002, Avg CpG Sum Difference: 0.1765\n",
            "Epoch 34/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1636\n",
            "Epoch 35/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1533\n",
            "Epoch 36/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1419\n",
            "Epoch 37/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1337\n",
            "Epoch 38/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1247\n",
            "Epoch 39/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1174\n",
            "Epoch 40/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1100\n",
            "Epoch 41/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.1034\n",
            "Epoch 42/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0975\n",
            "Epoch 43/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0919\n",
            "Epoch 44/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0868\n",
            "Epoch 45/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0815\n",
            "Epoch 46/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0774\n",
            "Epoch 47/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0729\n",
            "Epoch 48/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0690\n",
            "Epoch 49/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0654\n",
            "Epoch 50/50, Train Loss: 0.0001, Avg CpG Sum Difference: 0.0622\n"
          ]
        }
      ],
      "source": [
        "# training (you can modify the code below)\n",
        "model.train()\n",
        "model.zero_grad()\n",
        "for epoch in range(epoch_num):\n",
        "    total_loss = 0\n",
        "    total_cpg_diff = 0\n",
        "    for batch in train_data_loader:\n",
        "        # complete training loop\n",
        "        sequences, custom_labels, int_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(sequences)\n",
        "        loss = loss_fn(logits, custom_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate the sum of sigmoid outputs\n",
        "        sigmoid_outputs = torch.sigmoid(logits)\n",
        "        sum_sigmoid = torch.sum(sigmoid_outputs, dim=1) # Sum over the sequence length dimension\n",
        "\n",
        "        # Calculate the difference with integer labels\n",
        "        cpg_diff = torch.sum(torch.abs(sum_sigmoid - int_labels))\n",
        "        total_cpg_diff += cpg_diff.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_data_loader)\n",
        "    avg_cpg_diff = total_cpg_diff / len(train_data_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epoch_num}, Train Loss: {avg_loss:.4f}, Avg CpG Sum Difference: {avg_cpg_diff:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "Jjyp4qTan5nJ"
      },
      "outputs": [],
      "source": [
        "# eval (you can modify the code below)\n",
        "model.eval()\n",
        "\n",
        "res_gs = []\n",
        "res_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, custom_labels, int_labels in test_data_loader:\n",
        "        # TODO complete inference loop\n",
        "        logits = model(sequences)\n",
        "        sigmoid_outputs = torch.sigmoid(logits)\n",
        "        sum_sigmoid = torch.sum(sigmoid_outputs, dim=1)\n",
        "\n",
        "        res_gs.extend(int_labels)\n",
        "        res_pred.extend(sum_sigmoid.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1GY830Dn5nK",
        "outputId": "7b3f79bd-a620-44bf-d486-23db08b4d0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[  2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0  10   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0  37   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  70   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  92   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 107   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  85   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  50   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  27   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  19   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   9   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   3   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1]]\n"
          ]
        }
      ],
      "source": [
        "# TODO complete evaluation of the model\n",
        "# Diplay accuracy and confusion matrix\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "res_gs = np.array(res_gs)\n",
        "res_pred = np.array(res_pred)\n",
        "res_pred_rounded = np.round(res_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(res_gs, res_pred_rounded)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Display confusion matrix\n",
        "conf_matrix = confusion_matrix(res_gs, res_pred_rounded)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMrRf_aVDRJm"
      },
      "source": [
        "# Part 2: what if the DNA sequences are not the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s-81pD46n5nL"
      },
      "outputs": [],
      "source": [
        "# hint we will need following imports\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AKvG-MNuXJr9"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE HERE\n",
        "random.seed(13)\n",
        "\n",
        "# Use this for getting x label\n",
        "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128):\n",
        "    for i in range(n_seqs):\n",
        "        seq_len = random.randint(lb, ub)\n",
        "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
        "\n",
        "\n",
        "# Use this for getting y label\n",
        "def count_cpgs(seq: str) -> int:\n",
        "    cgs = 0\n",
        "    for i in range(0, len(seq) - 1):\n",
        "        dimer = seq[i:i+2]\n",
        "        # note that seq is a string, not a list\n",
        "        if dimer == \"CG\":\n",
        "            cgs += 1\n",
        "    return cgs\n",
        "\n",
        "\n",
        "# Alphabet helpers\n",
        "alphabet = 'NACGT'\n",
        "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
        "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
        "dna2int.update({\"pad\": 0})\n",
        "int2dna.update({0: \"<pad>\"})\n",
        "\n",
        "intseq_to_dnaseq = partial(map, int2dna.get)\n",
        "dnaseq_to_intseq = partial(map, dna2int.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4IM7_Ujvn5nM"
      },
      "outputs": [],
      "source": [
        "# TODO complete the task based on the change\n",
        "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
        "    # TODO prepared the training and test data\n",
        "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
        "    #step 1\n",
        "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
        "    #step 2\n",
        "    temp = [\"\".join(list(intseq_to_dnaseq(seq))) for seq in X_dna_seqs_train] # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
        "    #step3\n",
        "    y_dna_seqs = [count_cpgs(seq) for seq in temp] # use count_cpgs here to generate labels with temp generated in step2\n",
        "\n",
        "    return X_dna_seqs_train, y_dna_seqs\n",
        "\n",
        "\n",
        "min_len, max_len = 64, 128\n",
        "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
        "test_x, test_y = prepare_data(512, min_len, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ojsr8GxUn5nM"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, lists, labels) -> None:\n",
        "        self.lists = lists\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lists)\n",
        "\n",
        "\n",
        "# this will be a collate_fn for dataloader to pad sequence\n",
        "class PadSequence:\n",
        "    #TODO\n",
        "    def __call__(self, batch):\n",
        "        # batch is a list of (sequence, label) tuples\n",
        "        sequences, labels = zip(*batch)\n",
        "\n",
        "        # Get original lengths\n",
        "        lengths = torch.tensor([len(seq) for seq in sequences])\n",
        "\n",
        "        # Pad sequences\n",
        "        # pad_sequence pads to the longest sequence in the batch\n",
        "        # padding_value=0 because 0 is our padding token id\n",
        "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "\n",
        "        # Convert labels to a tensor\n",
        "        labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "        return padded_sequences, lengths, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some config\n",
        "LSTM_HIDDEN = 64\n",
        "EMBEDDING_DIM = 16\n",
        "LSTM_LAYER = 1\n",
        "batch_size = 32\n",
        "learning_rate = 0.005\n",
        "epoch_num = 50"
      ],
      "metadata": {
        "id": "lONLrN7sN6Vw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO complete the rest\n",
        "# create dataset\n",
        "train_dataset = MyDataset(train_x, train_y)\n",
        "test_dataset = MyDataset(test_x, test_y)\n",
        "\n",
        "# create dataloader with collate_fn\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=PadSequence())\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False, collate_fn=PadSequence())"
      ],
      "metadata": {
        "id": "QuOLxsrkNpJj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataloader\n",
        "for batch in train_data_loader:\n",
        "    sequences, lengths, labels = batch\n",
        "    print(sequences.shape, lengths.shape, labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQDyVykxN2Pd",
        "outputId": "47263826-5949-403b-c928-7aa2f9ab09c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 127]) torch.Size([32]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CpGPredictor2(torch.nn.Module):\n",
        "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size):\n",
        "        super(CpGPredictor2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        # Linear layer to predict the count from the last hidden state\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        embedded = self.embedding(x) # embedded shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Pack the padded sequences\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "\n",
        "        # Use the hidden state from the last time step of the last layer\n",
        "        # hidden shape: (num_layers, batch_size, hidden_dim)\n",
        "        # We want the last layer's hidden state: hidden[-1, :, :]\n",
        "        last_hidden_state = hidden[-1, :, :] # shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # Pass the last hidden state through the linear layer\n",
        "        logits = self.linear(last_hidden_state) # shape: (batch_size, 1)\n",
        "\n",
        "        # Squeeze the last dimension to get shape (batch_size,)\n",
        "        logits = logits.squeeze(-1)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "8sWUcK-nOaU5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size is 5 (N, A, C, G, T, <pad>)\n",
        "VOCAB_SIZE = 6\n",
        "\n",
        "model2 = CpGPredictor2(embedding_dim=EMBEDDING_DIM, hidden_dim=LSTM_HIDDEN, num_layers=LSTM_LAYER, vocab_size=VOCAB_SIZE)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "OMTVaU4eToez"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a6b23f0",
        "outputId": "73a76203-3ecf-4b31-dbaa-ad70d8ecef76"
      },
      "source": [
        "# training loop\n",
        "model2.train()\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    total_loss = 0\n",
        "    for sequences, lengths, labels in train_data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        logits = model2(sequences, lengths)\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_data_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epoch_num}, Train Loss: {avg_loss:.4f}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 4.7456\n",
            "Epoch 2/50, Train Loss: 3.6923\n",
            "Epoch 3/50, Train Loss: 3.6871\n",
            "Epoch 4/50, Train Loss: 3.6673\n",
            "Epoch 5/50, Train Loss: 3.6819\n",
            "Epoch 6/50, Train Loss: 3.6492\n",
            "Epoch 7/50, Train Loss: 3.6678\n",
            "Epoch 8/50, Train Loss: 3.6476\n",
            "Epoch 9/50, Train Loss: 3.6237\n",
            "Epoch 10/50, Train Loss: 3.6586\n",
            "Epoch 11/50, Train Loss: 3.6781\n",
            "Epoch 12/50, Train Loss: 3.7141\n",
            "Epoch 13/50, Train Loss: 3.6311\n",
            "Epoch 14/50, Train Loss: 3.5425\n",
            "Epoch 15/50, Train Loss: 2.7049\n",
            "Epoch 16/50, Train Loss: 1.1222\n",
            "Epoch 17/50, Train Loss: 0.4451\n",
            "Epoch 18/50, Train Loss: 0.1909\n",
            "Epoch 19/50, Train Loss: 0.0873\n",
            "Epoch 20/50, Train Loss: 0.0667\n",
            "Epoch 21/50, Train Loss: 0.0653\n",
            "Epoch 22/50, Train Loss: 0.0347\n",
            "Epoch 23/50, Train Loss: 0.0197\n",
            "Epoch 24/50, Train Loss: 0.0256\n",
            "Epoch 25/50, Train Loss: 0.0185\n",
            "Epoch 26/50, Train Loss: 0.0135\n",
            "Epoch 27/50, Train Loss: 0.0237\n",
            "Epoch 28/50, Train Loss: 0.0271\n",
            "Epoch 29/50, Train Loss: 0.0108\n",
            "Epoch 30/50, Train Loss: 0.0088\n",
            "Epoch 31/50, Train Loss: 0.0111\n",
            "Epoch 32/50, Train Loss: 0.0152\n",
            "Epoch 33/50, Train Loss: 0.0156\n",
            "Epoch 34/50, Train Loss: 0.0318\n",
            "Epoch 35/50, Train Loss: 0.0084\n",
            "Epoch 36/50, Train Loss: 0.0063\n",
            "Epoch 37/50, Train Loss: 0.0053\n",
            "Epoch 38/50, Train Loss: 0.0103\n",
            "Epoch 39/50, Train Loss: 0.0109\n",
            "Epoch 40/50, Train Loss: 0.0100\n",
            "Epoch 41/50, Train Loss: 0.0091\n",
            "Epoch 42/50, Train Loss: 0.0042\n",
            "Epoch 43/50, Train Loss: 0.0036\n",
            "Epoch 44/50, Train Loss: 0.0111\n",
            "Epoch 45/50, Train Loss: 0.0055\n",
            "Epoch 46/50, Train Loss: 0.0055\n",
            "Epoch 47/50, Train Loss: 0.0091\n",
            "Epoch 48/50, Train Loss: 0.0178\n",
            "Epoch 49/50, Train Loss: 0.0285\n",
            "Epoch 50/50, Train Loss: 0.0056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.eval()\n",
        "\n",
        "res_gs2 = []\n",
        "res_pred2 = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, lengths, labels in test_data_loader:\n",
        "        logits = model2(sequences, lengths)\n",
        "        res_gs2.extend(labels.tolist())\n",
        "        res_pred2.extend(logits.tolist())"
      ],
      "metadata": {
        "id": "A-N0D6pwWHF_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to numpy arrays\n",
        "res_gs2 = np.array(res_gs2)\n",
        "res_pred2 = np.array(res_pred2)\n",
        "res_pred_rounded2 = np.round(res_pred2)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(res_gs2, res_pred_rounded2)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Display confusion matrix\n",
        "conf_matrix = confusion_matrix(res_gs2, res_pred_rounded2)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VLnJja6-PVZ",
        "outputId": "67a3aed3-f15b-4893-aecc-dccc084bdcae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[12  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 55  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 81  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 93  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 84  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 93  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 47  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 31  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  9  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  4  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Models"
      ],
      "metadata": {
        "id": "2UqUVauo_pFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "save_model(model, \"model1.pth\")\n",
        "save_model(model2, \"model2.pth\")"
      ],
      "metadata": {
        "id": "Rq4-uR9G_bhe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model_instance, seq, lstm_model: bool):\n",
        "    model_instance.eval()\n",
        "    seq_tensor = torch.tensor([list(dnaseq_to_intseq(seq))], dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if not lstm_model:\n",
        "            print(seq_tensor.shape)\n",
        "            logits = model_instance(seq_tensor)\n",
        "            # Apply sigmoid and sum to get the predicted count\n",
        "            sigmoid_outputs = torch.sigmoid(logits)\n",
        "            predicted_count = torch.sum(sigmoid_outputs, dim=1).item()\n",
        "            return predicted_count\n",
        "        else:\n",
        "            lengths = torch.tensor([len(seq)])\n",
        "            logits = model_instance(seq_tensor, lengths)\n",
        "            # Squeeze the output and get the item\n",
        "            predicted_count = logits.squeeze(-1).item()\n",
        "\n",
        "            return predicted_count, round(predicted_count)"
      ],
      "metadata": {
        "id": "OQZQGJU0ADRJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'NACGT'"
      ],
      "metadata": {
        "id": "EZRhiN3ZD7g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model2, \"NNACTCGCGANTC\", lstm_model=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REoY2HQ7DhDU",
        "outputId": "87e70d4c-70b2-48af-d24b-28207eb0cb39"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.064582347869873, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(\"\".join([random.choice(\"NACGT\") for _ in range(128)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47z63Q_IFgq7",
        "outputId": "767baa0e-86f1-465d-e27e-6d810e5c53ad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, \"\".join([random.choice(\"NACGT\") for _ in range(128)]), lstm_model=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zb-lJJFpEF-u",
        "outputId": "e9689a46-74b8-4a2f-c171-e14345f90693"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-8513be290ab5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NACGT\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-69f385065c1b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model_instance, seq, lstm_model)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# Apply sigmoid and sum to get the predicted count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0msigmoid_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0eacb8d61a2a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# x shape: (batch_size, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# embedded shape: (batch_size, seq_len, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Prepare input for the linear layer: concatenate embeddings of prev and curr tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whXQYBMCEyyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}